{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB4a - NERC with Conditional Random Fields (CRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL\n",
    "\n",
    "### Credits\n",
    "\n",
    "The content of this notebook is an adaptation of:\n",
    "https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/\n",
    "\n",
    "which is itself based on:\n",
    "\n",
    "https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are going to use Conditional Random Fields (CRF) to train a Named Entity Recognition and Classifciation (NERC) system. CRF classifiers have been specifically succesful for this this task for several reasons:\n",
    "\n",
    "<ol>\n",
    "    <li>They can take a wide variety of features into account\n",
    "    <li>They exploit both sequences of words with their annotations and sequences of features into account to make predictions\n",
    "</ol>\n",
    "\n",
    "We can see the task of NERC as a special sequence annotation task, in which some tokens in a sentence fall outside named-entity expressions, while other are part of a named entity expression. As such it has similarities with part-of-speech tagging, phrase structure chunking but also with semantic classification when classifying a named-entity-phrase for some type: person, organisaiton, location, time expression, etc. Due to the nature of the task there is a wide range of features that can contribute but there is also a strong sequence depedency in that the features of one token predict the tags of the next token and vice versa. Like part-of-speech tagging, sequence dependencies typical are reflected with the boundaries of a sentence. That is why CRF models for NERC, typically use the sentence as a unit for representing features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "You first need to install the special sklearn-crfsuite which does not come with sklearn. Open a command line within the Anaconda install environment and run the next command:\n",
    "\n",
    ">pip install sklearn-crfsuite\n",
    "\n",
    "For evaluation of sequence tagging, we are going to use a pakage *seqeval* which was tested on CoNLL tasks:\n",
    "\n",
    "https://github.com/chakki-works/seqeval\n",
    "\n",
    "> pip install seqeval[cpu]\n",
    "\n",
    "\n",
    "To analyse the features used we also need another package:\n",
    "\n",
    ">pip install eli5\n",
    "\n",
    "See: https://eli5.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eli5 Fix\n",
    "\n",
    "The eli5 library is no longer supported, and in order to get it to work, you might need to modify two files which contain an outdated import.\n",
    "To do so, run eli5_patch.py from your terminal (while located in your working directory, run \"python eli5_patch.py\"). After that, the library should work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "We first present a formal model for the typical properties of the data that our classifier needs to annotate. If you are not familiar with the mathematical modeling of such problems, you can skip this subsection. The model helps explaining how a model can adapted to avoid overfitting to the trainign set by forcing it to down-rank certain features and generalise more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically represent the data as a sequence of words and as a sequence of tags, which are the output states of each word token in the sequence, i.e. being part of a named-entity expression or not.\n",
    "\n",
    "We denote the input sequence (the words in a sentence):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x = (x_1,\\dots, x_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of output states, i.e. the named entity tags, is represented as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s = (s_1,\\dots, s_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conditional random fields we model the conditional probability for a sequence *1..m*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(s_1,\\dots,s_m|x_1,\\dots,x_m)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this by defining a feature map that maps an entire input sequence *x* paired with an entire state sequence *s* to some d-dimensional feature vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\Phi(x_1,\\dots,x_m,s_1,\\dots,s_m)\\in\\mathbb{R}^d$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can model the probability as a log-linear model with the parameter vector `w`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(s|x; w) = \\frac{\\exp(w\\cdot\\Phi(x, s))}{\\sum_{s^\\prime} \\exp(w\\cdot\\Phi(x, s^\\prime))},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here *s'* ranges over all possible output sequences. For the estimation of *w*, we assume that we have a set of *n* labeled examples. Now we define the regularized log-likelihood function L:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(w) = \\sum_{i=1}^n \\log p(s^i|x^i; w) - \\frac{\\lambda_2}{2}\\|w\\|_2^2 - \\lambda_1 \\|w\\|_1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lambda terms force the parameter vector to be small in the respective norm. This penalizes the model complexity and is known as **regularization**. The parameters lambda_2 and lambda_1 allow us to control the extent of regularization. The parameter vector $w^*$ is then estimated as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^* = \\text{arg max}_{w\\in \\mathbb{R}^d} L(w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we estimated the vector $w^*$, we can find the most likely tag a sentence $s^*$ for a sentence x by\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s^* = \\text{arg max}_{s} p(s|x; w^*).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "#### Step 0: Install the needed modules\n",
    "1.`sklearn_crfsuite`\n",
    "\n",
    "Run `pip install sklearn_crfsuite` or \n",
    "\n",
    "`conda install -c derickl sklearn-crfsuite`\n",
    "\n",
    "\n",
    "2.`eli5`\n",
    "\n",
    "ELI5 is a Python package which helps to debug machine learning classifiers and explain their predictions. It provides support for the following machine learning frameworks and packages: scikit-learn.\n",
    "\n",
    "https://eli5.readthedocs.io/en/latest/overview.html\n",
    "\n",
    "Run `pip install eli5` or \n",
    "\n",
    "`conda install -c conda-forge eli5`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step I: Loading the data\n",
    "\n",
    "Now we want to apply this model. Letâ€™s start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load an entity data set in CSV format that is provided through Kaggle and which follows a specifically adapted IOB annotation format. You can download the data set and the documentation from the next URL:\n",
    "\n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus#ner_dataset.csv\n",
    "\n",
    "\n",
    "We use the pandas framework to load the CSV data as a table with columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapt the path to load your local copy of the data set\n",
    "data = pd.read_csv(\"...path_to_your.../ner_dataset.csv\", encoding=\"latin1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotation has 4 columns, where the final column has the named entity tags and the first column is special as it represents a sentence identifier that is given for the first token of a sentence:\n",
    "\n",
    "```\n",
    "Sentence: 3,They,PRP,O\n",
    ",marched,VBD,O\n",
    ",from,IN,O\n",
    ",the,DT,O\n",
    ",Houses,NNS,O\n",
    ",of,IN,O\n",
    ",Parliament,NN,O\n",
    ",to,TO,O\n",
    ",a,DT,O\n",
    ",rally,NN,O\n",
    ",in,IN,O\n",
    ",Hyde,NNP,B-geo\n",
    ",Park,NNP,I-geo\n",
    ",.,.,O\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas framework is very powerful and provides many different options for data manipulation and conversion. Please consult the online documentation for more details.\n",
    "\n",
    "We are going to use a specific method to fill data holes so that we get a uniform representation. More details are provided here: \n",
    "\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill NA/NaN values using the specified method.\n",
    "\n",
    "data = data.ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step II: Initial analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many rows we have in our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have over a million rows with tokens as data. This is quite a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the *data.head(10)* and *data.tail(10)* functions, we can inspect the start and the end of of the data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the last 10 rows of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have *47,959* sentences in our data set. For a CRF approach, sentences are the text units to model sequences of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As further analysis, we can make a set of all unique words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set(data[\"Word\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(words); n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 47959 sentences containing 35177 unique words. We need the sentences as a unit for the CRF approach which assumes that sentences have some predictive sequence of words and likewise tags.\n",
    "\n",
    "We will use a class called SentenceGetter to retrieve sentences with their labels. Don't worry about the details of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, we can get a list of all the values for the column with the part-of-speech values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = list(set(data[\"POS\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we extract the list of unique annotation tags, in this case the named-entity IOB tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(set(data[\"Tag\"].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to learn about the prior distribution of the tags. For this, we can use the list of tags and apply the *Counter* function to generate the frequency count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "label_counts = collections.Counter(list(data[\"Tag\"].values))\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that *O* is by far the most dominant tag. The other tags are less frequent, where the standard entity types *geo*, *tim*, *org* and *per* are more dominant than the special types *art*, *eve*, *nat* and *gpe*. Such data distributions are important to understand data biases of systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function retrieves from the data frame, a list of tuples for each separate sentence, where we defined the tuples as a set consisting of the word, the part-of-speech-tag and the entity-tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that processes the data into sentences\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = getter.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example sentence we get with our SentenceGetter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get all sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= sentences[3]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step III: Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we craft a set of features and prepare the dataset. We define some typical features for NERC: the actual word (lowecase), the word beginning and ending, word shape features and the part-of-speech information. If there is a preceding word (i>0), we add some properties of the preceding word. If there is a following word in the sentence (i < len(sent)-1), we add similar properties for the following word. A special feature is added for the first and last word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is a sentence as a structure show above \n",
    "#and and ith word from the sentence to return the features for that word\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "    \n",
    "    # data structure consisting of a feature name and value for the token\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(), # lower case variant of the token\n",
    "        'word[-3:]': word[-3:], #suffix of 3 characters\n",
    "        'word[-2:]': word[-2:], #suffix of 2 characters\n",
    "        'word.isupper()': word.isupper(), # initial captial\n",
    "        'word.istitle()': word.istitle(), # all words ini caps\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2], #first two characters of the PoS Tag\n",
    "    }\n",
    "    if i > 0:\n",
    "        # adding features for the word based on the previous word\n",
    "        word1 = sent[i-1][0] # previous word\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True # Beginning of sentence as a feature\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        # adding features for the word based on the next word\n",
    "        word1 = sent[i+1][0] # next word\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True # end of sentence as a feature\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code extracts features with our functions above. It also prepares all labels from the original dataset.\n",
    "First, try processing the full data (first two lines). If that fails, restart the kernel and try the bottom two lines instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = sentences[0]\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = [sent2features(s) for s in sentences]\n",
    "#y = [sent2labels(s) for s in sentences]\n",
    "\n",
    "#If your enviornment breaks here, it might be because of very large lists being held in memory. Try loading first 10000 examples with:\n",
    "X = [sent2features(s) for s in sentences[:10]]\n",
    "y = [sent2labels(s) for s in sentences[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the first data representation in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step IV: Initialize CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the algorithm. We use the conditional random field (CRF) implementation provided by sklearn-crfsuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "# different parameters are used for training\n",
    "# check https://sklearn-crfsuite.readthedocs.io/en/latest/api.html?highlight=CRF\n",
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=0.1, #The coefficient for L1 regularization.\n",
    "          c2=0.1, #The coefficient for L2 regularization.\n",
    "          max_iterations=100,\n",
    "          all_possible_transitions=False) #When True, CRFsuite generates transition features that associate all of possible label pairs, \n",
    "                                        #including ones that never occur. Suppose that the number of labels in the training data is L, this function will generate (L * L) transition features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have defined a instance *crf* to train and test on our data. We are going to use 5-fold cross-validation, which means that we keep 20% for testing and 80% for trining and repeat this 5 times so that each part of the data is tested once and used four times for training. We average of the tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn_crfsuite.metrics import flat_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the sklearn_crfsuite classification report to evaluate the tagger, because we are basically interested in precision, recall and the f1-score. These metrics are common in NLP tasks and if you are not familiar with these metrics, then check out the wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step V: Train and test the CRF algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *cross_val_predict* to do the cross-validation, this takes a while as we have over a million data points, defined a rich feature set and need to repeat it 5 times. It takes a few minutes on a pretty decent laptop to run the cross-validation. If you are not sure your machine can handle it or if you cannot wait. You could go back and apply the sentence and label extraction on a subset of the sentences, e.g:\n",
    "\n",
    "X = [sent2features(s) for s in sentences[:10000]]\n",
    "y = [sent2labels(s) for s in sentences[:10000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the model \"crf\", \n",
    "# given the feature representations of the sentences x and their labels y,\n",
    "# apply 5-folded cross classifcation, testing 5 times on 80% train and 20% test\n",
    "# this may take half an hour depending on the machine you are running it\n",
    "pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're getting \"AttributeError: 'CRF' object has no attribute 'keep_tempfiles', downgrade your scikit-learn package with: \n",
    "\n",
    "pip uninstall scikit-learn <br>\n",
    "conda install -c anaconda scikit-learn==0.23.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can run *flat_classification_report* function from sklearn_crfsuite to the *pred* variable to obtain the report per IOB tag on the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report shows that the performance varies considerably across the different types of entities. Also note that the class \"O\" has F1 of 97 and is the dominant class. The support is the number of samples of the true response that lie in that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step VI: Inspect features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about CRFs is, that we can look into the algorithm and visualize the transition probabilites from one tag to another. We also can see which features are important for predicting a certain tag. We use the eli5 library to perform the investigation: https://eli5.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse the features, we need to build a model according to the whole data set. For this, we need to call the *fit* function on our data *X* and tags *y* again. This will take a few minutes as well (unless you limited the data!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CRFsuite CRF models use two kinds of features: state features and transition features. Letâ€™s check their weights using eli5.explain_weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(crf, top=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first table shows the learned weights for the transition probabilities. We see for example that *B-art* is most likely followed by *I-art* (8.442), while *I-art* is never followed by *B-art* and also by none of the other *I* tags, which makes sense. Check the table for other regularities and to see if they make sense.\n",
    "\n",
    "The second table shows for each category features that contributed most positively. Here we see that the CRF is just memorizing a lot of words (we have not used any gazetteers for creating features). For example for the tag â€˜B-perâ€™, the algorithm remembers â€˜presidentâ€™ â€˜obamaâ€™. This is called overfitting. It works for this data but not for other data in which other presidents rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of evaluating the IOB tags at the token level, we can also evaluate the complete sequence of an entity phrase.\n",
    "For sequence evaluation, we are going to use the *seqeval* package which is specifically designed for sequence annotations. \n",
    "In our case, it will return scores for he complete phrases instead of the IOB tags for the tokens. It also ignores the \"O\" tag which is dominant.\n",
    "\n",
    "We use the function *precision_score*, *recall_score*, and *f1_score* from the *seqeval* package to get the overall sequence annotation results for the total set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(\"precision-score: {:.1%}\".format(precision_score(y, pred)))\n",
    "print(\"recall-score: {:.1%}\".format(recall_score(y, pred)))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(y, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *seqeval* package also provides an option to derive a specific classification report for the entity types at phrase level instead of the token level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the results for the complete sequences is somewhat lower than for the token level annotation. Also note that the \"O\" tags are ignored. On the other hand, the overall macro averaged results are somewhat higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step VII: Tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome that CRF is memorizing words, we can tune the parameters, especially the regularization parameters of the CRF algorithm. The c1 and c2 parameter of the CRF algorithm are the regularization parameters \\lambda_1 and \\lambda_2. While c1 weights the l_1 regularization, the c2 parameter weights the l_2 regularization. We now limit the number of features used by enforcing sparsity on the parameter vector w. To do this we increase the l_1-regularization parameter c1. Reducing the number of features prevents the system from overfitting. If we regularize CRF more, we can expect that only features which are generic will remain, and memoized tokens will go. With L1 regularization (c1 parameter) coefficients of most features should be driven to zero. Letâ€™s check what effect does regularization have on CRF weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = CRF(algorithm='lbfgs',\n",
    "          c1=10, #L1 regularization is now set to 100\n",
    "          c2=0.1,\n",
    "          max_iterations=20,\n",
    "          all_possible_transitions=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note!\n",
    "The next command will take another half an hour to carry out the training and testing 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = cross_val_predict(estimator=crf, X=X, y=y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the details at the IOB tag level, we use again the flat_classification function from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = flat_classification_report(y_pred=pred, y_true=y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the evaluation results are not really better than before. For example B-per now scores 0.76P and 0.7R, while it scored  84P and 81R before. We also see that the macro average results score lower overall.\n",
    "\n",
    "But let's look at the features before we jump to conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect the features again, we need to call the *fit* function again. Take another break in case you did not limit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we look again at the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.show_weights(crf, top=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see, that the model stops to rely on words and uses the context more, as it generalizes better is more useful over multiple training instances. This is an effect of the l_1-regularization. Again looking at *B-per* and *I-per*, we see that the names dropped out and that parts-of-speech and words such as \"mr\" and \"president\" remain as the top scoring features.\n",
    "\n",
    "On regularization: \"Regularization is a technique to discourage the complexity of the model. It does this by penalizing the loss function. This helps to solve the overfitting problem.\"\n",
    "\n",
    "In particular, L1-regularization acts as a feature selector, simply removing some of the features. You can read more on regularization [here](https://medium.com/datadriveninvestor/l1-l2-regularization-7f1b4fe948f2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can thus conclude that although the model seems to perform less than before it is still a better model because it did not overfit on the names of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For entity phrase evaluation, we use the functions from the *seqeval* package which is specifically designed for sequence annotations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"precision-score: {:.1%}\".format(precision_score(y, pred)))\n",
    "print(\"recall-score: {:.1%}\".format(recall_score(y, pred)))\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(y, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarkably, the sequence evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original notebook on which this notebook was based can be found here:\n",
    "\n",
    "https://github.com/TeamHG-Memex/sklearn-crfsuite/blob/master/docs/CoNLL2002.ipynb\n",
    "\n",
    "It describes a similar process to build CRF-NERC classifier from the CoNLL-2002 dataset, which has Spanish and Dutch texts. You can follow this notebook to create your own NERC system for these languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
